{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9176753,"sourceType":"datasetVersion","datasetId":5546019}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\ndataset_path = '/kaggle/input/brain-cancer/Brain Cancer -  MRI dataset/Brain_Cancer raw MRI data/Brain_Cancer'\ntrain_output_path = '/kaggle/working/train'\ntest_output_path = '/kaggle/working/test'\nval_output_path = '/kaggle/working/validation'\n\nmanual_seed = 42\n\n# Function to create directories\ndef create_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n# Function to copy files\ndef files_copying(file_list, src_dir, dest_dir):\n    for file in file_list:\n        src_file = os.path.join(src_dir, file)\n        dest_file = os.path.join(dest_dir, file)\n        shutil.copy(src_file, dest_file)\n\n# List all class directories\nclass_directories = os.listdir(dataset_path)\n\n# Create output directories\ncreate_directory(train_output_path)\ncreate_directory(val_output_path)\ncreate_directory(test_output_path)\n\n# Split the dataset\nfor class_dir in class_directories:\n    class_path = os.path.join(dataset_path, class_dir)\n    image_files = os.listdir(class_path)\n    \n    if len(image_files) == 0:\n        print(f\"No images found in {class_path}, skipping this class.\")\n        continue\n    \n    train_files, val_test_files = train_test_split(image_files, train_size=0.8, random_state=manual_seed)\n    val_files, test_files = train_test_split(val_test_files, train_size=0.5, random_state=manual_seed)\n    \n    # Create class directories: train, test, validation\n    create_directory(os.path.join(train_output_path, class_dir))\n    create_directory(os.path.join(val_output_path, class_dir))\n    create_directory(os.path.join(test_output_path, class_dir))\n    \n    # Copy files\n    files_copying(train_files, class_path, os.path.join(train_output_path, class_dir))\n    files_copying(val_files, class_path, os.path.join(val_output_path, class_dir))\n    files_copying(test_files, class_path, os.path.join(test_output_path, class_dir))\n\nrandom.seed(manual_seed)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T19:45:35.513838Z","iopub.execute_input":"2024-09-09T19:45:35.514182Z","iopub.status.idle":"2024-09-09T19:45:56.833900Z","shell.execute_reply.started":"2024-09-09T19:45:35.514154Z","shell.execute_reply":"2024-09-09T19:45:56.833037Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Flatten,Dropout\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:45:56.835451Z","iopub.execute_input":"2024-09-09T19:45:56.835744Z","iopub.status.idle":"2024-09-09T19:46:09.130220Z","shell.execute_reply.started":"2024-09-09T19:45:56.835719Z","shell.execute_reply":"2024-09-09T19:46:09.129399Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-09-09 19:45:58.635596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-09 19:45:58.635699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-09 19:45:58.772887: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load_Dataset","metadata":{}},{"cell_type":"code","source":"train_data = '/kaggle/working/train'\ntest_data = '/kaggle/working/test'\nval_data = '/kaggle/working/validation'\n\nIMG_WIDTH, IMG_HEIGHT = 256,256\ninput_shape = (IMG_WIDTH, IMG_HEIGHT, 3)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.131230Z","iopub.execute_input":"2024-09-09T19:46:09.131701Z","iopub.status.idle":"2024-09-09T19:46:09.136247Z","shell.execute_reply.started":"2024-09-09T19:46:09.131674Z","shell.execute_reply":"2024-09-09T19:46:09.135357Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Data_Generators","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\nvalidation_datagen = ImageDataGenerator()\n\n#train data generator\ntrain_datagen_augmented = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.20,\n    horizontal_flip = True,\n    vertical_flip = False,\n    fill_mode = 'nearest'\n)\n\ntrain_generator = train_datagen_augmented.flow_from_directory(\n    train_data,\n    target_size = (IMG_WIDTH, IMG_HEIGHT),\n    batch_size = 10,\n    class_mode = 'categorical'\n)\n\n#test data generator\ntest_datagen_augmented = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip = False,\n    fill_mode = 'nearest'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data,\n    target_size = (IMG_WIDTH, IMG_HEIGHT),\n    batch_size = 8,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\n#validation generator\nvalidation_datagen_augmented = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip = False,\n    fill_mode = 'nearest'\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    val_data,\n    target_size = (IMG_WIDTH,IMG_HEIGHT),\n    batch_size = 8,\n    class_mode = 'categorical',\n    shuffle = 'True',\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.137416Z","iopub.execute_input":"2024-09-09T19:46:09.137694Z","iopub.status.idle":"2024-09-09T19:46:09.340172Z","shell.execute_reply.started":"2024-09-09T19:46:09.137671Z","shell.execute_reply":"2024-09-09T19:46:09.339484Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 4844 images belonging to 3 classes.\nFound 607 images belonging to 3 classes.\nFound 605 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class_indices = train_generator.class_indices\nprint(class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.342968Z","iopub.execute_input":"2024-09-09T19:46:09.343511Z","iopub.status.idle":"2024-09-09T19:46:09.347732Z","shell.execute_reply.started":"2024-09-09T19:46:09.343487Z","shell.execute_reply":"2024-09-09T19:46:09.346855Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'brain_glioma': 0, 'brain_menin': 1, 'brain_tumor': 2}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#number of image in train dataset\n\nclasses = os.listdir(train_data)\nfor class_name in classes:\n    class_path = os.path.join(train_data, class_name)\n    num_images = len(os.listdir(class_path))\n    print(class_name, num_images)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.348943Z","iopub.execute_input":"2024-09-09T19:46:09.349200Z","iopub.status.idle":"2024-09-09T19:46:09.361308Z","shell.execute_reply.started":"2024-09-09T19:46:09.349178Z","shell.execute_reply":"2024-09-09T19:46:09.360443Z"},"trusted":true},"outputs":[{"name":"stdout","text":"brain_menin 1603\nbrain_glioma 1603\nbrain_tumor 1638\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#number of images in test dataset\n\nclasses = os.listdir(test_data)\nfor class_name in classes:\n    class_path = os.path.join(test_data, class_name)\n    num_images = len(os.listdir(class_path))\n    print(class_name, num_images)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.362388Z","iopub.execute_input":"2024-09-09T19:46:09.362676Z","iopub.status.idle":"2024-09-09T19:46:09.368805Z","shell.execute_reply.started":"2024-09-09T19:46:09.362654Z","shell.execute_reply":"2024-09-09T19:46:09.367941Z"},"trusted":true},"outputs":[{"name":"stdout","text":"brain_menin 201\nbrain_glioma 201\nbrain_tumor 205\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#number of images in validation dataset\n\nclasses = os.listdir(val_data)\nfor class_name in classes:\n    class_path = os.path.join(val_data, class_name)\n    num_images = len(os.listdir(class_path))\n    print(class_name, num_images)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.369836Z","iopub.execute_input":"2024-09-09T19:46:09.370086Z","iopub.status.idle":"2024-09-09T19:46:09.380887Z","shell.execute_reply.started":"2024-09-09T19:46:09.370064Z","shell.execute_reply":"2024-09-09T19:46:09.380029Z"},"trusted":true},"outputs":[{"name":"stdout","text":"brain_menin 200\nbrain_glioma 200\nbrain_tumor 205\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Update the filepath to end with '.keras'\ncheckpoint_path = \"/kaggle/working/Brain_cancer/model_best.keras\"\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=False,\n                                                 save_best_only=True,  # Save only the best model\n                                                 monitor=\"val_accuracy\",   # Monitor validation accuracy\n                                                 mode=\"max\",           # Save the model when validation accuracy is maximized\n                                                 verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.382288Z","iopub.execute_input":"2024-09-09T19:46:09.382589Z","iopub.status.idle":"2024-09-09T19:46:09.389073Z","shell.execute_reply.started":"2024-09-09T19:46:09.382545Z","shell.execute_reply":"2024-09-09T19:46:09.388209Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"checkpoint_path","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.390208Z","iopub.execute_input":"2024-09-09T19:46:09.390997Z","iopub.status.idle":"2024-09-09T19:46:09.400299Z","shell.execute_reply.started":"2024-09-09T19:46:09.390967Z","shell.execute_reply":"2024-09-09T19:46:09.399449Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/Brain_cancer/model_best.keras'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.resnet import ResNet50, ResNet101 , ResNet152\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n\nfrom tensorflow.keras.models import Sequential, Model\n\n\ndef create_model(summary=True):\n    # apply transfer learning\n    new_input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n    base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=new_input) ##MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=new_input)\n    # add new classifier layers\n    flat1 = Flatten()(base_model.layers[-1].output)\n    output = Dense(3, activation='softmax')(flat1)  # Change to 1 for binary classification\n    # define new model\n    model = Model(inputs=base_model.inputs, outputs=output)\n    # Modify loss function to 'weighted_binary_crossentropy'\n    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    if summary:\n        print(model.summary())\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.401356Z","iopub.execute_input":"2024-09-09T19:46:09.401674Z","iopub.status.idle":"2024-09-09T19:46:09.420037Z","shell.execute_reply.started":"2024-09-09T19:46:09.401649Z","shell.execute_reply":"2024-09-09T19:46:09.419208Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = create_model()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:09.420990Z","iopub.execute_input":"2024-09-09T19:46:09.421246Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/applications/mobilenet_v3.py:452: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n\u001b[1m4334752/4334752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"#training of model\n\nhistory = model.fit(\n   train_generator,\n   batch_size = 10,\n   epochs = 15,\n   validation_data = validation_generator,\n   callbacks = [cp_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:46:12.599544Z","iopub.execute_input":"2024-09-09T19:46:12.599915Z","iopub.status.idle":"2024-09-09T20:04:51.868961Z","shell.execute_reply.started":"2024-09-09T19:46:12.599883Z","shell.execute_reply":"2024-09-09T20:04:51.868232Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/485\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:54:16\u001b[0m 51s/step - accuracy: 0.3000 - loss: 2.7522","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1725911224.263122     126 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1725911224.321394     126 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1725911224.341386     126 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.4821 - loss: 1.6133\nEpoch 1: val_accuracy improved from -inf to 0.68430, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 203ms/step - accuracy: 0.4824 - loss: 1.6123 - val_accuracy: 0.6843 - val_loss: 0.9003\nEpoch 2/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7043 - loss: 0.8650\nEpoch 2: val_accuracy improved from 0.68430 to 0.72231, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.7043 - loss: 0.8646 - val_accuracy: 0.7223 - val_loss: 0.7684\nEpoch 3/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7823 - loss: 0.6202\nEpoch 3: val_accuracy improved from 0.72231 to 0.74050, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.7824 - loss: 0.6201 - val_accuracy: 0.7405 - val_loss: 0.7869\nEpoch 4/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8271 - loss: 0.4950\nEpoch 4: val_accuracy improved from 0.74050 to 0.74876, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 140ms/step - accuracy: 0.8271 - loss: 0.4950 - val_accuracy: 0.7488 - val_loss: 0.7929\nEpoch 5/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8445 - loss: 0.4410\nEpoch 5: val_accuracy improved from 0.74876 to 0.76529, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.8446 - loss: 0.4409 - val_accuracy: 0.7653 - val_loss: 0.7173\nEpoch 6/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8735 - loss: 0.3824\nEpoch 6: val_accuracy improved from 0.76529 to 0.76694, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.8735 - loss: 0.3824 - val_accuracy: 0.7669 - val_loss: 0.7376\nEpoch 7/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8850 - loss: 0.3465\nEpoch 7: val_accuracy improved from 0.76694 to 0.77190, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 140ms/step - accuracy: 0.8850 - loss: 0.3465 - val_accuracy: 0.7719 - val_loss: 0.7260\nEpoch 8/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8840 - loss: 0.3214\nEpoch 8: val_accuracy improved from 0.77190 to 0.79339, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 142ms/step - accuracy: 0.8840 - loss: 0.3213 - val_accuracy: 0.7934 - val_loss: 0.6151\nEpoch 9/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8958 - loss: 0.2948\nEpoch 9: val_accuracy did not improve from 0.79339\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 140ms/step - accuracy: 0.8959 - loss: 0.2947 - val_accuracy: 0.7835 - val_loss: 0.6789\nEpoch 10/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9039 - loss: 0.2785\nEpoch 10: val_accuracy improved from 0.79339 to 0.80000, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.9039 - loss: 0.2785 - val_accuracy: 0.8000 - val_loss: 0.5515\nEpoch 11/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9118 - loss: 0.2532\nEpoch 11: val_accuracy did not improve from 0.80000\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 140ms/step - accuracy: 0.9119 - loss: 0.2531 - val_accuracy: 0.7868 - val_loss: 0.6170\nEpoch 12/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9108 - loss: 0.2422\nEpoch 12: val_accuracy improved from 0.80000 to 0.80331, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.9108 - loss: 0.2422 - val_accuracy: 0.8033 - val_loss: 0.5568\nEpoch 13/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9240 - loss: 0.2013\nEpoch 13: val_accuracy improved from 0.80331 to 0.82645, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 142ms/step - accuracy: 0.9240 - loss: 0.2014 - val_accuracy: 0.8264 - val_loss: 0.5023\nEpoch 14/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9287 - loss: 0.2054\nEpoch 14: val_accuracy improved from 0.82645 to 0.87107, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 141ms/step - accuracy: 0.9287 - loss: 0.2054 - val_accuracy: 0.8711 - val_loss: 0.3508\nEpoch 15/15\n\u001b[1m484/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9272 - loss: 0.2053\nEpoch 15: val_accuracy improved from 0.87107 to 0.91240, saving model to /kaggle/working/Brain_cancer/model_best.keras\n\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 140ms/step - accuracy: 0.9272 - loss: 0.2052 - val_accuracy: 0.9124 - val_loss: 0.2461\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Save the training history\ninitial_epoch = 0  # or the actual initial epoch of the first training session\nsaved_history = {\n    'loss': history.history['loss'],\n    'accuracy': history.history['accuracy'],\n    'val_loss': history.history['val_loss'],\n    'val_accuracy': history.history['val_accuracy'],\n    # Add other metrics as needed\n}\nnp.save(\"/kaggle/working/InceptionResNetV2_saved_history.npy\", saved_history)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:04:51.873711Z","iopub.execute_input":"2024-09-09T20:04:51.874368Z","iopub.status.idle":"2024-09-09T20:04:51.879449Z","shell.execute_reply.started":"2024-09-09T20:04:51.874340Z","shell.execute_reply":"2024-09-09T20:04:51.878617Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# Set the path to the saved model\nsaved_model_path = 'kaggle/working/Brain_cancer/model_best.keras'\n\n# Check if the model file exists\nif tf.io.gfile.exists(saved_model_path):\n    # Load the entire model including its weights and configurations\n    loaded_model = load_model(saved_model_path)\n    print(\"Model loaded successfully.\")\nelse:\n    print(\"No checkpoint file found at:\", saved_model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:04:51.880649Z","iopub.execute_input":"2024-09-09T20:04:51.880915Z","iopub.status.idle":"2024-09-09T20:04:51.892775Z","shell.execute_reply.started":"2024-09-09T20:04:51.880893Z","shell.execute_reply":"2024-09-09T20:04:51.891903Z"},"trusted":true},"outputs":[{"name":"stdout","text":"No checkpoint file found at: kaggle/working/Brain_cancer/model_best.keras\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# from matplotlib.lines import Line2D\n# from matplotlib.legend_handler import HandlerLine2D\n# import numpy as np\n\n# # Plot training & validation loss values\n# plt.figure(figsize=(10, 6))\n\n# # Plot Loss\n# train_loss, = plt.plot(previous_history['loss'], label='Train Loss', color='blue')\n# val_loss, = plt.plot(previous_history['val_loss'], label='Validation Loss', color='orange')\n# train_accuracy, = plt.plot(previous_history['accuracy'], label='Train Accuracy',  color='green')\n# val_accuracy, = plt.plot(previous_history['val_accuracy'], label='Validation Accuracy', color='red')\n# # Add a title with specified font properties\n# plt.title('Model Performance during Training', fontdict={'family': 'Serif', 'weight': 'bold', 'size': 12},pad=10)\n# # Set x-axis label with specified font properties\n# plt.xlabel('No. of Epochs', fontdict={'family': 'Serif', 'weight': 'bold', 'size': 12})\n\n# # Set x-axis ticks font properties\n# #plt.xticks(np.linspace(0, len(history.history['loss']), num=6), fontname='Serif', weight='bold')\n\n# plt.xticks(np.linspace(0, 100, num=11), fontname='Serif', weight='bold')\n\n\n# # Set y-axis ticks font properties\n# plt.yticks(np.linspace(0, 4, num=5), fontname='Serif', weight='bold')\n\n# # Set the x-axis and y-axis limits\n# #plt.xlim(0, len(history.history['loss']))\n\n# plt.xlim(0, 100)\n# plt.ylim(0, 4)\n\n# # Define custom legend lines with desired line properties\n# legend_lines = [\n#     Line2D([0], [0], color='blue', lw=3),          # Train Loss\n#     Line2D([0], [0], color='orange', lw=3),       # Validation Loss\n#     Line2D([0], [0], color='green', lw=3),        # Train Accuracy\n#     Line2D([0], [0], color='red', lw=3)           # Validation Accuracy\n# ]\n\n# # Place legend outside the graph by adjusting bbox_to_anchor and specifying it to be outside the axes\n# plt.legend(legend_lines, ['Train Loss', 'Validation Loss', 'Train Accuracy', 'Validation Accuracy'],\n#            loc='lower center', bbox_to_anchor=(0.5, 1.1), ncol=5,\n#            prop={'family': 'Serif', 'weight': 'bold', 'size': 8}, frameon=False,\n#            handler_map={Line2D: HandlerLine2D(numpoints=5)})\n\n# # Adjust padding between x-axis label and x-axis ticks\n# plt.gca().xaxis.labelpad = 10  # Change the value as needed to adjust the space\n\n# # Remove top and right spines\n# plt.gca().spines['top'].set_visible(False)\n# plt.gca().spines['right'].set_visible(False)\n# # Adjust layout to prevent cropping\n# plt.tight_layout()\n# plt.savefig('/kaggle/working/Brain_cancer_accuracy_graph.pdf')  # Save as pdf format\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:04:51.894153Z","iopub.execute_input":"2024-09-09T20:04:51.894582Z","iopub.status.idle":"2024-09-09T20:04:51.911724Z","shell.execute_reply.started":"2024-09-09T20:04:51.894536Z","shell.execute_reply":"2024-09-09T20:04:51.910850Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# Set the path to the saved model\nsaved_model_path = '/kaggle/working/Brain_cancer/model_best.keras'\n\n# Check if the model file exists\nif tf.io.gfile.exists(saved_model_path):\n    # Load the entire model including its weights and configurations\n    loaded_model = load_model(saved_model_path)\n    print(\"Model loaded successfully.\")\nelse:\n    print(\"No checkpoint file found at:\", saved_model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:04:51.912638Z","iopub.execute_input":"2024-09-09T20:04:51.912921Z","iopub.status.idle":"2024-09-09T20:04:53.187008Z","shell.execute_reply.started":"2024-09-09T20:04:51.912898Z","shell.execute_reply":"2024-09-09T20:04:53.186092Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_acc = loaded_model.evaluate(test_generator)\nprint(f\"Test Accuracy: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:04:53.188314Z","iopub.execute_input":"2024-09-09T20:04:53.188645Z","iopub.status.idle":"2024-09-09T20:05:02.057160Z","shell.execute_reply.started":"2024-09-09T20:04:53.188619Z","shell.execute_reply":"2024-09-09T20:05:02.056235Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 69ms/step - accuracy: 0.9331 - loss: 0.1870\nTest Accuracy: 0.9093904495239258\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss, jaccard_score\n# Predict labels for the test set\npredictions = loaded_model.predict(test_generator)\npredicted_classes = np.argmax(predictions, axis=1)  # Get the index of the highest probability class\ntrue_classes = test_generator.classes\n\n# Display some of the predicted and true classes\nprint(\"Predicted Classes:\", predicted_classes[-10:])  # Display first 10 predicted classes\nprint(\"True Classes:\", true_classes[-10:])  # Display first 10 true classes\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T20:05:02.058186Z","iopub.execute_input":"2024-09-09T20:05:02.058451Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Accuracy: {accuracy_score(true_classes, predicted_classes)}\")\nprint(f\"Precision: {precision_score(true_classes, predicted_classes,average='weighted')}\")\nprint(f\"Recall: {recall_score(true_classes, predicted_classes,average='weighted')}\")\nprint(f\"F1 Score: {f1_score(true_classes, predicted_classes,average='weighted')}\")\n#print(f\"Log Loss: {log_loss(true_classes, predicted_classes)}\")\nprint(f\"Jaccard Score: {jaccard_score(true_classes, predicted_classes,average='weighted')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate confusion matrix\nconf_matrix = confusion_matrix(true_classes, predicted_classes)\n\n# Plot confusion matrix\nplt.figure(figsize=(8,6))\n# Define the custom palette\ncustom_palette = sns.color_palette(palette='GnBu')  # Modify the number based on the number of classes in the dataset\n\n# Define custom font dictionary for title and labels\nfont = {'family': 'Serif', 'weight': 'bold', 'size': 12}\nfont2 = {'family': 'Serif', 'weight': 'bold', 'size': 10}\n\n# Create heatmap with annotations and colormap\nheatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=custom_palette, vmin=0, vmax=950,\n                      xticklabels=['brain_giloma', 'brain_menin', 'brain_tumor'], \n                      yticklabels=['brain_giloma', 'brain_menin', 'brain_tumor'],\n                      annot_kws={\"family\": \"Serif\", 'color':'black', 'weight': 'bold', 'size': 13})\n\n# Set x and y labels with the custom font dictionary\nheatmap.set_xlabel('Predicted Labels', fontdict=font2)\nheatmap.set_ylabel('Target Labels', fontdict=font2)\n\n# Set font properties for tick labels on both axes\nheatmap.set_xticklabels(heatmap.get_xticklabels(), fontname='Serif', fontsize=12)\nheatmap.set_yticklabels(heatmap.get_yticklabels(), fontname='Serif', fontsize=12)\n\n# Create a color bar to indicate the scale\ncbar = heatmap.collections[0].colorbar\ncbar.set_label('Count', fontdict=font)\ncbar.ax.tick_params(labelsize=10)\n\n# Adjust padding between x-axis label and x-axis ticks\nplt.gca().xaxis.labelpad = 10  # Change the value as needed to adjust the space\nplt.xticks(rotation=0)\nplt.yticks(rotation=0)\n\n# Adjust layout to prevent cropping\nplt.tight_layout()\n\n# Save as pdf format\nplt.savefig('/kaggle/working/MobileNetV3.pdf')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\n\n# Load and preprocess the image\nimg_path = \"/kaggle/working/test/brain_glioma/brain_glioma_0571.jpg\"  # Replace with the path to your image\nimg = image.load_img(img_path, target_size=(256, 256))  # Adjust target_size according to your model input size (256, 256)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)  # Create batch dimension\nimg_array = tf.keras.applications.resnet50.preprocess_input(img_array)  # Adjust preprocessing based on your model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Function to generate Grad-CAM heatmap\ndef generate_gradcam(model, img_array, last_conv_layer_name):\n    grad_model = Model(\n        inputs=[model.inputs],\n        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if isinstance(predictions, list):\n            predictions = predictions[0]  # Access the first output if it's a list\n        loss = predictions[:, tf.argmax(predictions[0])]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    conv_outputs = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(conv_outputs)\n\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = cv2.resize(heatmap.numpy(), (img_array.shape[2], img_array.shape[1]))  # Resize heatmap to image size\n    return heatmap\n\n# Function to overlay the heatmap on the original image\ndef overlay_heatmap_on_image(img_path, heatmap, alpha=0.6):\n    img = cv2.imread(img_path)\n    \n    # Ensure the heatmap and the image have the same number of channels\n    if len(img.shape) == 2:  # Grayscale image\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    \n    # Resize the heatmap to match the image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Convert heatmap to 3 channels\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Overlay the heatmap on the original image\n    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n\n# Example usage for Grad-CAM\nimg_path = \"/kaggle/working/test/brain_glioma/brain_glioma_0571.jpg\"  # Replace with the path to your image\nimg = image.load_img(img_path, target_size=(256, 256))  # Adjust target_size according to your model input size (256, 256)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)  # Create batch dimension\nimg_array = tf.keras.applications.resnet50.preprocess_input(img_array)  # Adjust preprocessing based on your model\n\nlast_conv_layer_name = \"expanded_conv_10_project\"  # Adjust if necessary\n\nheatmap = generate_gradcam(loaded_model, img_array, last_conv_layer_name)\noverlay_heatmap_on_image(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_gradcam_plus_plus(model, img_array, last_conv_layer_name):\n    grad_model = Model(inputs=[model.inputs], outputs=[model.get_layer(last_conv_layer_name).output, model.output])\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if isinstance(predictions, list):\n            predictions = predictions[0]  # Access the first output if it's a list\n        loss = predictions[:, tf.argmax(predictions[0])]\n\n    grads = tape.gradient(loss, conv_outputs)\n    grads_squared = tf.square(grads)\n    grads_cubed = tf.pow(grads, 3)\n\n    sum_grads = tf.reduce_sum(conv_outputs, axis=(1, 2))\n    alpha_num = grads_squared\n    alpha_denom = grads_squared * 2.0 + sum_grads * grads_cubed\n    alphas = alpha_num / (alpha_denom + tf.keras.backend.epsilon())\n    \n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    conv_outputs = conv_outputs[0]\n    weighted_sum = conv_outputs @ weights[..., tf.newaxis]\n    heatmap = tf.squeeze(weighted_sum)\n\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = cv2.resize(heatmap.numpy(), (img_array.shape[2], img_array.shape[1]))  # Resize heatmap to image size\n    return heatmap\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_scorecam(model, img_array, last_conv_layer_name, alpha=0.5):\n    grad_model = Model(inputs=[model.inputs], outputs=[model.get_layer(last_conv_layer_name).output])\n    \n    conv_outputs = grad_model(img_array)\n    conv_outputs = conv_outputs[0]\n    \n    heatmap = np.zeros((img_array.shape[1], img_array.shape[2]), dtype=np.float32)  # Initialize heatmap with image size\n    \n    for i in range(conv_outputs.shape[-1]):\n        upsampled_output = cv2.resize(conv_outputs[..., i].numpy(), (img_array.shape[2], img_array.shape[1]))\n        upsampled_output = np.expand_dims(upsampled_output, axis=-1)  # Expand to make it single-channel\n        \n        # Create a 3-channel version of the upsampled output for prediction\n        upsampled_output_rgb = np.repeat(upsampled_output, 3, axis=-1)\n        \n        # No need to preprocess again since the image is already normalized and in the correct format\n        score = model.predict(np.expand_dims(upsampled_output_rgb, axis=0))\n        class_score = score[:, tf.argmax(score[0])]  # Get the score of the predicted class\n        \n        # Resize class_score to the original image size\n        resized_score = cv2.resize(class_score, (img_array.shape[2], img_array.shape[1]))\n        \n        heatmap += resized_score * upsampled_output[..., 0]\n\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)  # Normalize the heatmap\n    return heatmap\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_layercam(model, img_array, last_conv_layer_name):\n    grad_model = Model(inputs=[model.inputs], outputs=[model.get_layer(last_conv_layer_name).output, model.output])\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if isinstance(predictions, list):\n            predictions = predictions[0]  # Access the first output if it's a list\n        loss = predictions[:, tf.argmax(predictions[0])]\n\n    grads = tape.gradient(loss, conv_outputs)\n    grads = tf.maximum(grads, 0)\n\n    conv_outputs = conv_outputs[0]\n    weights = grads / (tf.reduce_sum(grads, axis=(0, 1, 2), keepdims=True) + tf.keras.backend.epsilon())\n\n    weighted_conv = conv_outputs * weights\n    heatmap = tf.reduce_sum(weighted_conv, axis=-1)\n\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = cv2.resize(heatmap.numpy(), (img_array.shape[2], img_array.shape[1]))  # Resize heatmap to image size\n    return heatmap\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def overlay_heatmap_on_image(img_path, heatmap, alpha=0.6):\n    img = cv2.imread(img_path)\n\n    # Ensure the heatmap is a valid NumPy array\n    if not isinstance(heatmap, np.ndarray):\n        heatmap = heatmap.numpy()\n\n    # Handle the case where the heatmap has multiple channels\n    if len(heatmap.shape) == 3 and heatmap.shape[2] > 1:\n        heatmap = np.mean(heatmap, axis=-1)  # Average across the channels to create a 2D heatmap\n\n    # Now heatmap should be a 2D array\n    heatmap = np.uint8(255 * heatmap)\n\n    # Ensure the image and heatmap have the same number of channels\n    if len(img.shape) == 2:  # Grayscale image\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n    # Resize the heatmap to match the image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply the color map\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Overlay the heatmap on the original image\n    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grad-CAM++\nheatmap_gradcam_pp = generate_gradcam_plus_plus(loaded_model, img_array, last_conv_layer_name)\noverlay_heatmap_on_image(img_path, heatmap_gradcam_pp)\n\n# Score-CAM\nheatmap_scorecam = generate_scorecam(loaded_model, img_array, last_conv_layer_name)\noverlay_heatmap_on_image(img_path, heatmap_scorecam)\n\n# Layer-CAM\nheatmap_layercam = generate_layercam(loaded_model, img_array, last_conv_layer_name)\noverlay_heatmap_on_image(img_path, heatmap_layercam)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}